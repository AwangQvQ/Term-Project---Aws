#add package
!pip install pandas pyarrow sodapy
!pip install fsspec s3fs boto3
#TLC Trip Record Data ï¼ˆoriginal
#https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page
#open resourse 
#https://data.cityofnewyork.us/Transportation/2021-Yellow-Taxi-Trip-Data/m6nq-qud6
#manual
#https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf


#Login to S3
# To work with Amazon S3 storage, set the following variables using
# your AWS Access Key and Secret Key
# Set the Region to where your files are stored in S3.
import os
from pyspark.sql import SparkSession
access_key = 'xxxxxxx'
secret_key = 'xxxxxxx'
# Set the environment variables so boto3 can pick them up later.
# Do not modify the next three lines of code
os.environ['AWS_ACCESS_KEY_ID'] = access_key
os.environ['AWS_SECRET_ACCESS_KEY'] = secret_key
encoded_secret_key = secret_key.replace("/", "%2F").replace("+", "%2B")
# Set aws_region to where your S3 bucket was created
aws_region = "us-east-2"
# Update the Spark options to work with our AWS Credentials
# Do not modify the next four lines
sc._jsc.hadoopConfiguration().set("fs.s3a.access.key", access_key)
sc._jsc.hadoopConfiguration().set("fs.s3a.secret.key", secret_key)
sc._jsc.hadoopConfiguration().set("fs.s3a.endpoint", "s3." + aws_region +
".amazonaws.com")
sc._jsc.hadoopConfiguration().set("fs.s3a.bucket."+4130'+".endpoint.region"
, aws_region



#combine monthly to yearly file A
import pandas as pd
s3_bucket = '4130'
output_file_path = 'yellow_tripdata_2019-A.parquet'.format(s3_bucket)
# List of Parquet file URLs
parquet_urls_2021 = [
    "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-01.parquet",
    "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-02.parquet",
    "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-03.parquet",
    "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-04.parquet",
    "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-05.parquet",
    "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-06.parquet",
]
# Read Parquet files from URLs into Pandas DataFrames
dfs = [pd.read_parquet(url, engine='pyarrow') for url in parquet_urls_2021]
# Combine DataFrames (concatenate vertically)
combined_df = pd.concat(dfs, ignore_index=True)
# Save the combined DataFrame as a Parquet file to S3
combined_df.to_parquet(output_file_path, engine='pyarrow')
print("Combined file saved to:", output_file_path)



#target a file to clean 
from pyspark.sql.functions import col, isnan, when, count, udf
file_path = 'yellow_tripdata_2022-B.parquet'
sample_set = spark.read.parquet(file_path, sep='\t', header=True, inferSchema=True)
sample_set.show(20, truncate=False)

# Check data for outliers, nulls etc.
from pyspark.sql.functions import *
columns_to_check = ["airport_fee", "mta_tax", "congestion_surcharge", "total_amount", "payment_type"]
sdf.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in columns_to_check] ).show()

# Look at statistics for some specific columns
sample_set.select("passenger_count", "trip_distance", "total_amount").summary("count", "min",
"max", "mean").show()
total_rows = sample_set.count()
print("Total rows in this file:",total_rows,"rows.")
#take very long if you have a big dataset (11012023)
# sample_set.summary().show()


#check columns one by one 
#set a table
peek_table = sample_set.groupBy("airport_fee").count()
peek_table = peek_table.orderBy("airport_fee")
# Show the ordered table
peek_table.show(300)

# Check to see if some of the columns have NULL values
sample_set.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in
["trip_distance", "passenger_count"]] ).show()



#Taxi Zone csv turn to parquet
#Change column "PULocationID" and "DOLocationID" to area/borough name
#https://data.cityofnewyork.us/Transportation/NYC-Taxi-Zones/d3c5-ddgc

from pyspark.sql import SparkSession
# Create a Spark session
spark = SparkSession.builder.appName("CSV to Parquet Conversion").getOrCreate()
# Specify the path to your CSV file
csv_file_path = 's3://4130projectaw/landing/taxi_zones.csv'
# Read the CSV file into a Spark DataFrame
df = spark.read.csv(csv_file_path, header=True, inferSchema=True)
df = df.drop("Shape_Leng","the_geom","Shape_Area")
df = df.select("LocationID", "zone", "borough")
# Specify the path for the output Parquet file
location_path = 's3://4130projectaw/raw/taxi_zones.parquet'
# Write the DataFrame to a Parquet file
df.write.parquet(location_path)
# Show the content of the converted Parquet file
letter_Location = spark.read.parquet(location_path)
letter_Location.show(300)
