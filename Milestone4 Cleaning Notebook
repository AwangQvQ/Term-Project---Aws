#clean data from landing then write it to raw
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Create a Spark session
spark = SparkSession.builder.appName("Read and Clean").getOrCreate()

file_path = 's3://4130projectaw/landing/yellow_tripdata_2022-B.parquet'
sample_set = spark.read.parquet(file_path, sep='\t', header=True, inferSchema=True)
location_path = 's3://4130projectaw/raw/taxi_zones.parquet'
letter_Location = spark.read.parquet(location_path)

# Perform a left join to update PUzone and PUborough in sample_set based on LocationID
result_PU = sample_set.join(letter_Location, sample_set['PULocationID'] == letter_Location['LocationID'], 'left_outer') \
    .drop(letter_Location['LocationID'])  # Drop the duplicate LocationID column


# Add new columns "PUzone" and "PUborough" based on values in letter_Location
result_PU = result_PU.withColumn("PUzone", col("zone")).withColumn("PUborough", col("borough"))
result_PU = result_PU.drop("zone", "borough")

# Show the updated DataFrame
#result_PU.show(10, truncate=False)

# Perform a left join to update DOzone and DOborough in result_PU based on DOLocationID
result_PU = result_PU.join(letter_Location.alias("loc_do"), result_PU['DOLocationID'] == col("loc_do.LocationID"), 'left_outer') \
    .drop("loc_do.LocationID")  # Drop the duplicate LocationID column

# Add new columns "DOzone" and "DOborough" based on values in letter_Location
result_PU = result_PU.withColumn("DOzone", col("loc_do.zone")).withColumn("DOborough", col("loc_do.borough"))
result_PU = result_PU.drop("zone", "borough","LocationID","store_and_fwd_flag")
result_PU = result_PU.dropna()

# Show the updated DataFrame
result_PU.show(10, truncate=False)

# Write the DataFrame to a Parquet file
raw_path = 's3://4130projectaw/raw/cleaned_yellow_tripdata_2022-B.parquet'
result_PU.write.parquet(raw_path)

# Check data for outliers, nulls etc.
from pyspark.sql.functions import *
raw_path = 's3://4130projectaw/raw/cleaned_yellow_tripdata_2023-01.parquet'
test_set = spark.read.parquet(raw_path, sep='\t', header=True, inferSchema=True)

columns_to_check = ["airport_fee", "mta_tax", "congestion_surcharge", "total_amount", "payment_type"]
test_set.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in columns_to_check] ).show()

test_set.select("vendorID", "RatecodeID", "payment_type", "airport_fee", "mta_tax", "congestion_surcharge").summary().show()

# Drop the airport fee because many of the values are null - not all taxi trips go to an airport
# Drop the VendorID because it seems to be unqiue for each taxi trip
# Drop the RatecodeID
# Drop the store_and_fwd_flag

columns_to_drop = ['vendorID', 'RatecodeID', 'store_and_fwd_flag', 'congestion_surcharge', 'improvement_surcharge']
test_set = test_set.drop(*columns_to_drop)

               
# Fill missing values with a default value or drop records with missing values
test_set = test_set.na.fill(0)  # You can replace 0 with the default value you want


# Write the cleaned data as a Parquet file to the raw folder
raw_folder_path = 's3://4130projectaw/raw/cleaned_yellow_tripdata_2023-01.parquet'
test_set.write.parquet(raw_folder_path, mode='overwrite')
